{
 "metadata": {
  "name": "",
  "signature": "sha256:c1dd6294addc20b3d2cdb2d5a2d9a52c00a42d16dc6a617f7747994ea0a6bae0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import libraries\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "import pandas.io.data as web\n",
      "import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn import svm\n",
      "from sklearn import preprocessing\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.decomposition import PCA, RandomizedPCA\n",
      "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score, accuracy_score, log_loss\n",
      "\n",
      "class Model(object):\n",
      "\n",
      "    def __init__(self, Xs, ys, model=None, scale=True, components=None):\n",
      "        \n",
      "        self.Xs = Xs \n",
      "        self.scaler = None\n",
      "        self.pca = None\n",
      "        \n",
      "        if scale:\n",
      "            # normalize data, convert to z-scores\n",
      "            self.scaler = preprocessing.StandardScaler().fit(Xs)\n",
      "            self.Xs = self.scaler.transform(Xs)\n",
      "            \n",
      "        if components:\n",
      "            # reduce dimensionality of Xs\n",
      "            self.pca = PCA(n_components=components)\n",
      "            self.Xs = self.pca.fit_transform(self.Xs)\n",
      "            \n",
      "        self.ys = ys\n",
      "        self.model = model\n",
      "    \n",
      "    def preprocess_Xs(self, Xs):\n",
      "        # apply trained/fit model to new data set\n",
      "        if self.scaler:\n",
      "            # input data will be scaled\n",
      "            Xs = self.scaler.transform(Xs)\n",
      "        if self.pca:\n",
      "            # input dimensionality will be reduced\n",
      "            Xs = self.pca.transform(Xs)\n",
      "        return Xs\n",
      "    \n",
      "    def predict(self, X_test):\n",
      "        X_test = self.preprocess_Xs(X_test)\n",
      "        predicted = self.model.predict(X_test)\n",
      "        return predicted\n",
      "    \n",
      "    def simple_split_test(self):    \n",
      "        # split data set into train and test sets, keeping most recent data for test set\n",
      "        # return model accuracy\n",
      "        sample_count = int(math.floor(len(master)*0.25))\n",
      "        X_train = self.Xs[:-sample_count-1]\n",
      "        y_train = self.ys[:-sample_count-1]\n",
      "        X_test = self.Xs[-sample_count:]\n",
      "        y_test = self.ys[-sample_count:]\n",
      "        \n",
      "        self.model.fit(X_train, y_train)\n",
      "        predicted = self.model.predict(X_test)\n",
      "        \n",
      "        scores = {\n",
      "        'explained_variance_score': '%0.2f' % explained_variance_score(y_test, predicted),\n",
      "        'mean_absolute_error': '%0.2f' % mean_absolute_error(y_test, predicted),\n",
      "        'mean_sqrd_err_reg_loss': '%0.2f' % mean_squared_error(y_test, predicted),\n",
      "        'r2_score': '%0.2f' % r2_score(y_test, predicted),\n",
      "        }\n",
      "        # fit model to full training set\n",
      "        self.model.fit(self.Xs, self.ys)\n",
      "        \n",
      "        return scores\n",
      "    \n",
      "    def dim_reduc_chart(self):\n",
      "        pca = PCA()\n",
      "        pca.fit(self.Xs)\n",
      "        \n",
      "        fig = plt.figure()\n",
      "        chart = fig.add_subplot(111)\n",
      "        chart.plot(pca.explained_variance_, linewidth=2)\n",
      "        chart.axis('tight')\n",
      "        chart.set_xlabel('N Components')\n",
      "        chart.set_ylabel('Explained Variance')\n",
      "        chart.set_title('Dimension Explanation')\n",
      "        \n",
      "    def iter_cross_val(self, classification=False):\n",
      "        # performs multiple cross-validation tests on data and returns averaged accuracy\n",
      "        # data set is kept in chronological order and devided into equal parts, or folds\n",
      "        # if data set is split into folds numbered: [1,2,3,4,5]\n",
      "        # the training / testing split follows this pattern:\n",
      "        # train[1] / test[2]\n",
      "        # train[1,2] / test[3]\n",
      "        # train[1,2,3] / test[4]\n",
      "        # train[1,2,3,4] / test[5]\n",
      "        \n",
      "        number_folds = 5\n",
      "        k = int(np.floor(float(self.Xs.shape[0]) / number_folds))\n",
      "        \n",
      "        if not classification:\n",
      "            bank_explained_variance_score = np.zeros(number_folds-1)\n",
      "            bank_mean_absolute_error = np.zeros(number_folds-1)\n",
      "            bank_mean_sqrd_err_reg_loss = np.zeros(number_folds-1)\n",
      "            bank_r2_score = np.zeros(number_folds-1)\n",
      "        else:\n",
      "            bank_accuracy = np.zeros(number_folds-1)\n",
      "            bank_log_loss = np.zeros(number_folds-1)\n",
      "        \n",
      "        for i in range(2, number_folds + 1):\n",
      "            split = float(i-1)/i\n",
      "            X = self.Xs[:(k*i)]\n",
      "            y = self.ys[:(k*i)]\n",
      "            index = int(np.floor(X.shape[0] * split))\n",
      "            \n",
      "            # folds used to train the model        \n",
      "            X_trainFolds = X[:index]        \n",
      "            y_trainFolds = y[:index]\n",
      "\n",
      "            # fold used to test the model\n",
      "            X_testFold = X[(index + 1):]\n",
      "            y_testFold = y[(index + 1):]\n",
      "            \n",
      "            self.model.fit (X_trainFolds, y_trainFolds)\n",
      "            \n",
      "            predicted = self.model.predict(X_testFold)\n",
      "\n",
      "            if not classification:\n",
      "                bank_explained_variance_score[i-2] = explained_variance_score(y_testFold, predicted)\n",
      "                bank_mean_absolute_error[i-2] = mean_absolute_error(y_testFold, predicted)\n",
      "                bank_mean_sqrd_err_reg_loss[i-2] = mean_squared_error(y_testFold, predicted)\n",
      "                bank_r2_score[i-2] = r2_score(y_testFold, predicted)\n",
      "            else:\n",
      "                predicted_proba = self.model.predict_proba(X_testFold)\n",
      "                #score = self.model.score(X_testFold, y_testFold)    \n",
      "                #scores[i-2] = score\n",
      "                bank_accuracy[i-2] = accuracy_score(y_testFold, predicted)\n",
      "                bank_log_loss[i-2] = log_loss(y_testFold, predicted_proba)\n",
      "\n",
      "        # fit model to full training set\n",
      "        self.model.fit(self.Xs, self.ys)\n",
      "        \n",
      "        if not classification:\n",
      "            return {\n",
      "            'explained_variance_score': '%0.2f (+/- %0.2f)' % (bank_explained_variance_score.mean(), bank_explained_variance_score.std() * 2), \n",
      "            'mean_absolute_error': '%0.2f (+/- %0.2f)' % (bank_mean_absolute_error.mean(), bank_mean_absolute_error.std() * 2), \n",
      "            'mean_sqrd_err_reg_loss': '%0.2f (+/- %0.2f)' % (bank_mean_sqrd_err_reg_loss.mean(), bank_mean_sqrd_err_reg_loss.std() * 2), \n",
      "            'r2_score': '%0.2f (+/- %0.2f)' % (bank_r2_score.mean(), bank_r2_score.std() * 2), \n",
      "            }\n",
      "        else:\n",
      "            return {\n",
      "                'accuracy_score': '%0.2f (+/- %0.2f)' % (bank_accuracy.mean(), bank_accuracy.std() * 2),\n",
      "                'log_loss': '%0.2f (+/- %0.2f)' % (bank_log_loss.mean(), bank_log_loss.std() * 2),\n",
      "            }\n",
      "    \n",
      "    def iter_cross_val_pred_probs_threshold(self, threshold=0):\n",
      "        # performs multiple cross-validation tests on data and returns indices by classification - 0 or 1\n",
      "        # threshold setting ensures only predictions of a certain confidence will be returned\n",
      "        \n",
      "        # data set is kept in chronological order and devided into equal parts, or folds\n",
      "        # if data set is split into folds numbered: [1,2,3,4,5]\n",
      "        # the training / testing split follows this pattern:\n",
      "        # train[1] / test[2]\n",
      "        # train[1,2] / test[3]\n",
      "        # train[1,2,3] / test[4]\n",
      "        # train[1,2,3,4] / test[5]\n",
      "        \n",
      "        number_folds = 5\n",
      "        k = int(np.floor(float(self.Xs.shape[0]) / number_folds))\n",
      "        \n",
      "        \n",
      "        increase = []\n",
      "        decrease = []\n",
      "\n",
      "        for i in range(2, number_folds + 1):\n",
      "            split = float(i-1)/i\n",
      "            X = self.Xs[:(k*i)]\n",
      "            y = self.ys[:(k*i)]\n",
      "            index = int(np.floor(X.shape[0] * split))\n",
      "            \n",
      "            # folds used to train the model        \n",
      "            X_trainFolds = X[:index]        \n",
      "            y_trainFolds = y[:index]\n",
      "\n",
      "            # fold used to test the model\n",
      "            X_testFold = X[(index):]\n",
      "            y_testFold = y[(index):]\n",
      "            \n",
      "            self.model.fit (X_trainFolds, y_trainFolds)\n",
      "        \n",
      "            predicted_proba = self.model.predict_proba(X_testFold)\n",
      "   \n",
      "            inc = np.where(predicted_proba[:,1] > 0.5 + threshold)[0]\n",
      "            inc += index\n",
      "            increase.append(inc)\n",
      "            \n",
      "            dec = np.where(predicted_proba[:,1] < 0.5 - threshold)[0]\n",
      "            dec += index\n",
      "            decrease.append(dec)\n",
      "           \n",
      "\n",
      "        # fit model to full training set\n",
      "        self.model.fit(self.Xs, self.ys)\n",
      "        \n",
      "        \n",
      "        return {\n",
      "                'increase': increase,\n",
      "                'decrease': decrease,\n",
      "                }\n",
      "\n",
      "    \n",
      "def test_classifier(model, test_set, y_col, X_cols):\n",
      "    # test classifier model on new data\n",
      "    ys = test_set[y_col].values\n",
      "    Xs = test_set[X_cols].values\n",
      "    Xs = model.preprocess_Xs(Xs)\n",
      "    predicted = model.model.predict(Xs)\n",
      "    print \"Accuracy: %f\" % (accuracy_score(ys, predicted))\n",
      "    predicted_proba = model.model.predict_proba(Xs)\n",
      "    print \"Log Loss: %f\" % (log_loss(ys, predicted_proba))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 234
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load source code data into a Pandas dataframe\n",
      "data = pd.read_csv('source/GDP_USA.csv', index_col=0, parse_dates=True)\n",
      "GDP_USA = pd.DataFrame(data)\n",
      "data = pd.read_csv('source/usa_aggr_monthly.csv', index_col=0)\n",
      "usa_aggr_monthly = pd.DataFrame(data)\n",
      "data = pd.read_csv('source/usa_cons_monthly.csv', index_col=0)\n",
      "usa_cons_monthly = pd.DataFrame(data)\n",
      "data = pd.read_csv('source/usa_cred_monthly.csv', index_col=0)\n",
      "usa_cred_monthly = pd.DataFrame(data)\n",
      "data = pd.read_csv('source/usa_real_monthly.csv', index_col=0)\n",
      "usa_real_monthly = pd.DataFrame(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# combine dataframes into one master dataframe, indexed by date\n",
      "master = pd.concat([\n",
      "                    GDP_USA.ix[:,:], \n",
      "                    usa_aggr_monthly.ix[:,:],\n",
      "                    usa_cons_monthly.ix[:,:],\n",
      "                    usa_cred_monthly.ix[:,:],\n",
      "                    usa_real_monthly.ix[:,:],\n",
      "                    ], \n",
      "                   axis=1)\n",
      "\n",
      "# convert date index to python datetime\n",
      "from dateutil.parser import parse\n",
      "dates = master.index.values\n",
      "dates = dates * 100 + 1\n",
      "dates = np.array(map(str, dates))\n",
      "dates = np.array(map(parse, dates))\n",
      "master.index = dates\n",
      "\n",
      "# simple series to visualize forecast data\n",
      "forecasts = master.ix[:,0]\n",
      "forecasts = forecasts.dropna(how='any')\n",
      "\n",
      "# remove rows where Y (forecast GDP) is NaN\n",
      "master = master.ix[forecasts.index]\n",
      "\n",
      "# remove features with missing data\n",
      "master_wo_missing = master.dropna(axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 230
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lagged features\n",
      "shifted = master_wo_missing.shift(1)\n",
      "shifted = shifted.fillna(method='bfill')\n",
      "shifted = shifted.fillna(method='ffill')\n",
      "\n",
      "# one period change ratio\n",
      "change = master_wo_missing / shifted\n",
      "change = change.replace([np.inf, -np.inf], 0)\n",
      "change = change.replace(np.nan, 0)\n",
      "\n",
      "change.rename(columns=lambda x: \"%s-change\" % (x), inplace=True)\n",
      "shifted.rename(columns=lambda x: \"%s-lag\" % (x), inplace=True)\n",
      "\n",
      "# build exponentially weighted moving average for and each feature\n",
      "ewma = pd.stats.moments.ewma\n",
      "EMOV_n = ewma(master_wo_missing, com=5)\n",
      "EMOV_n.rename(columns=lambda x: \"%s-EWMA\" % (x), inplace=True)\n",
      "\n",
      "# build exponentially weighted moving average for changes in forecasts\n",
      "EMOV_change = ewma(change, com=5)\n",
      "EMOV_change.rename(columns=lambda x: \"%s-EWMA-change\" % (x), inplace=True)\n",
      "\n",
      "master_w_changes_and_lags = pd.concat([\n",
      "                                        master_wo_missing, \n",
      "                                        shifted, \n",
      "                                        change,\n",
      "                                        EMOV_n,\n",
      "                                        EMOV_change,\n",
      "                                        ],\n",
      "                                        axis=1)\n",
      "\n",
      "# remove the meta features relating to GDP Forecasts\n",
      "master_w_changes_and_lags_no_GDP = master_w_changes_and_lags.drop('USA GDP Revisions-EWMA', 1)\n",
      "master_w_changes_and_lags_no_GDP = master_w_changes_and_lags_no_GDP.drop('USA GDP Revisions-change', 1)\n",
      "master_w_changes_and_lags_no_GDP = master_w_changes_and_lags_no_GDP.drop('USA GDP Revisions-lag', 1)\n",
      "#master_w_changes_and_lags_no_GDP = master_w_changes_and_lags_no_GDP.drop('USA GDP Revisions-EWMA-change', 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 231
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# add feature to indicate whether forecast increased or decreased since previous period\n",
      "fwd_GDP = master_w_changes_and_lags_no_GDP.ix[:,'USA GDP Revisions'].shift(-1)\n",
      "GDP = master_w_changes_and_lags_no_GDP.ix[:,'USA GDP Revisions']\n",
      "increase = fwd_GDP - GDP\n",
      "increase = increase.dropna(how='any')\n",
      "increase = increase.map(lambda x: 1 if x > 0 else 0)\n",
      "increase.name = \"GDP Forecast Increase\"\n",
      "\n",
      "\n",
      "master_dir_change = pd.concat([\n",
      "                    increase,\n",
      "                    master_w_changes_and_lags_no_GDP.ix[increase.index], \n",
      "                    ], \n",
      "                   axis=1)\n",
      "master_dir_change = master_dir_change.drop('USA GDP Revisions', 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 232
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# select subset of rows for testing and model verification\n",
      "import math\n",
      "sample_count = int(math.floor(len(master_dir_change)*0.20))\n",
      "test_set = master_dir_change.ix[-sample_count:]\n",
      "train_set = master_dir_change.ix[:-sample_count-1]\n",
      "\n",
      "# prepare for building time series features\n",
      "cols = master_dir_change.columns\n",
      "y_col = cols[0]\n",
      "X_cols = cols[1:]\n",
      "ys = train_set[y_col].values\n",
      "Xs = train_set[X_cols].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 217
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "# implement simple grid search to find best hyper-parameters\n",
      "Cs = np.array([.01, .1, 1, 10,])\n",
      "Dims = np.array([None])\n",
      "for c in Cs:\n",
      "    for d in Dims:\n",
      "        print \"C: %s, Dimensions: %s\" % (c, d)\n",
      "        model = Model(Xs, ys, LogisticRegression(C=c), components=d)\n",
      "        scores = model.iter_cross_val(classification=True)\n",
      "        print \"accuracy score: %s, log_loss: %s\" % (scores['accuracy_score'] ,scores['log_loss'])\n",
      "        print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C: 0.01, Dimensions: None\n",
        "accuracy score: 0.66 (+/- 0.21), log_loss: 0.60 (+/- 0.14)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "C: 0.1, Dimensions: None\n",
        "accuracy score: 0.61 (+/- 0.15), log_loss: 0.76 (+/- 0.20)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "C: 1.0, Dimensions: None\n",
        "accuracy score: 0.60 (+/- 0.16), log_loss: 1.15 (+/- 0.27)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "C: 10.0, Dimensions: None\n",
        "accuracy score: 0.60 (+/- 0.22), log_loss: 1.67 (+/- 0.39)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test fitted model on new data\n",
      "model = Model(Xs, ys, LogisticRegression(C=.01))\n",
      "model.model.fit(model.Xs, ys)\n",
      "test_classifier(model, test_set, y_col, X_cols)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.627119\n",
        "Log Loss: 0.670860\n"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find securities' correlations with GDP forecasts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = master_dir_change.index[0]\n",
      "end = master_dir_change.index[-1] + datetime.timedelta(weeks=5)\n",
      "df = web.DataReader(\"AAPL\", 'yahoo', start, end)\n",
      "\n",
      "value = df[['Close','Volume']].prod(axis=1).resample('M', how=\"sum\")\n",
      "tot_vol = df.Volume.resample('M', how=\"sum\")\n",
      "vwap = value / tot_vol\n",
      "change = vwap / vwap.shift(-1) - 1\n",
      "change = change.dropna(how='any')\n",
      "# change naming of index to match GDP forecast indexing\n",
      "change.index = (change.index + datetime.timedelta(days=1)).shift(-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = Model(Xs, ys, LogisticRegression(C=.01),)\n",
      "preds = model.iter_cross_val_pred_probs_threshold(threshold=.2)\n",
      "preds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 240,
       "text": [
        "{'decrease': [array([56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 71, 76]),\n",
        "  array([ 98, 104, 106, 107, 108, 127, 128, 129, 132, 134, 135, 136]),\n",
        "  array([144, 160]),\n",
        "  array([190, 191, 192, 198, 199, 200, 201, 202, 208, 214, 216, 217, 220,\n",
        "         221, 222, 223, 224, 228, 229, 230, 231, 232])],\n",
        " 'increase': [array([52, 75]),\n",
        "  array([], dtype=int64),\n",
        "  array([142, 145, 146, 147, 148, 149, 151, 163, 165, 166, 167, 168, 169,\n",
        "         170, 171, 174, 176]),\n",
        "  array([194, 234])]}"
       ]
      }
     ],
     "prompt_number": 240
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "change.ix[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 248,
       "text": [
        "0.056519776836762459"
       ]
      }
     ],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}